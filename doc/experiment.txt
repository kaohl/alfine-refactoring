# Data generation (refactorings)

Two shell scripts are used for doing refactoring experiments.

1) `experiment-prepare.sh` pass all received arguments to the framework `export`
   command. The `experiment/assets` and `experiment/assets/lib` folders are used
   for source and binary archives, respectively.

2) `experiment-run.sh` produce 100 versions of the project exported using
   `experiment-prepare.sh` where each version contains a single refactoring
   of the specified type. Refactorings are selected from the set of variable
   source artifacts as specified by the project configuration at the time of
   running `experiment-prepare.sh`. Transformed artifacts are copied together
   with `output.log` (produced when running the refactoring framework) into
   their own folder, a subdirectory of `experiment/output`, if the refactoring
   was successful in the sense that the framework produced output.

# Benchmarking and data analysis

For each project version, do the following:

1) Compile project version,
2) Execute benchmark N times to get runtime average and standard deviation.

For each measurement, plot the deviation from original runtime average including
boxes for standard deviation and look for non-overlapping boxes. We produce one
plot per project and refactoring type.

Plot reference runtime as the line y=0, including two horizontal lines
indicating original standard deviation. Plot a mark for each versions'
runtime average, including a box for its associated standard deviation.

r    := reference runtime average of original project
std  := reference runtime standard deviation of original project
ri   := runtime average of project version i
stdi := runtime standard deviation of project version i
di   := runtime average deviation from original average

(d0=(r0 - r), [-std0,+std0]),
(d1=(r1 - r), [-std1,+std1]),
...
