# Data generation (refactorings)

Two shell scripts are used for doing refactoring experiments.

1) `experiment-prepare.sh` pass all received arguments to the framework `export`
   command. The `experiment/assets` and `experiment/assets/lib` folders are used
   for source and binary archives, respectively.

2) `experiment-run.sh` produce 100 versions of the project exported using
   `experiment-prepare.sh` where each version contains a single refactoring
   of the specified type. Refactorings are selected from the set of variable
   source artifacts as specified by the project configuration at the time of
   running `experiment-prepare.sh`. Transformed artifacts are copied together
   with `output.log` (produced when running the refactoring framework) into
   their own folder, a subdirectory of `experiment/output`, if the refactoring
   was successful in the sense that the framework produced output.

# Benchmarking and data analysis

For each project version, do the following:

1) Compile project version,
2) Execute benchmark N times to get runtime average and standard deviation.

For each measurement, plot the deviation from original runtime average including
boxes for standard deviation and look for non-overlapping boxes. We produce one
plot per project and refactoring type.

Plot reference runtime as the line y=0, including two horizontal lines
indicating original standard deviation. Plot a mark for each versions'
runtime average, including a box for its associated standard deviation.

r    := reference runtime average of original project
std  := reference runtime standard deviation of original project
ri   := runtime average of project version i
stdi := runtime standard deviation of project version i
di   := runtime average deviation from original average

(d0=(r0 - r), [-std0,+std0]),
(d1=(r1 - r), [-std1,+std1]),
...



# About selecting distinct opportunities:
# One way to do this is to use the same shuffle-seed but drop one extra
# refactoring each iteration, this way, we are guaranteed to get
# distinct refactorings because the list is deterministic and each
# opportunity occurs once. The only problem with this is that
# we do not know how many refactorings to drop if one fails which
# means that we would redo the same refactoring a number of times
# before reaching a new one. A fix would be to skip an integer multiple
# of `limit` number of refactorings. This would pick the first
# successful refactoring in each interval of `limit` refactorings
# or fail if a refactoring can not be found within the interval.
# `limit` would have to be chosen depending on the number of
# available refactorings (we risk running out of refactorings).
#
# A third way is to pick a random shuffle-seed and then pick the first
# successful refactoring for each seed, terminating if limit is reached.
# This method is probably best because it is simpler and does not risk
# scewing the probability for selecting any particular refactoring by
# introducing yet another selection mechanism. However, to guarantee
# uniqueness, we have to compare patch files (inspect them manually?).
# The probability for collisions should be pretty low though especially
# for rename-refactorings.

